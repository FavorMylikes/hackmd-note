---
layout: single
title:  "MeshNet"
date:   "2023-08-21 21:05:19 +0800"
categories: AI
header:
  teaser: https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820161208.png
---

### Introduction

#### WGAN-GP: Wasserstein GAN with Gradient Penalty

- [Paper](https://arxiv.org/abs/1704.00028v3)

### 论文要点

- Wasserstein GAN
  - WGAN在训练过程中可能遇到不稳定性和收敛困难的问题，而WGAN-GP通过引入梯度惩罚项来替代权重裁剪，能够有效提升GAN的稳定性和训练效果。
  - WGAN-GP中，通过对批次中真实样本和生成样本之间直线上的点进行采样，并对生成器的梯度进行惩罚来实现Lipschitz约束。这种方法能够在不进行任何超参数调整的情况下，稳定地训练各种GAN架构，包括具有101层ResNet的图像生成模型和连续生成器的语言模型。
  - 在实验中，通过对CIFAR-10和LSUN卧室数据集进行训练和生成，展示了WGAN-GP相对于传统权重裁剪方法在训练速度和样本质量上的改进。论文还对200个随机架构进行了训练并对比其性能，结果显示WGAN-GP能够成功训练大多数架构。
- 需要理解KL散度，JS散度（KL的对称版）
- GAN
  - D(Y, θ)：其中相对熵被用神经网络训练(希望尽可能大-网络可以区分)
  - G(X, θ)：希望两个分布尽可能相似(希望尽可能小)
- 

### Reference

- [互怼的艺术：从零直达WGAN-GP](https://kexue.fm/archives/4439)
  - [公众号：PaperWeekly 第41期 | 互怼的艺术：从零直达 WGAN-GP](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247484880&idx=1&sn=4b2e976cc715c9fe2d022ff6923879a8&chksm=96e9da50a19e5346307b54f5ce172e355ccaba890aa157ce50fda68eeaccba6ea05425f6ad76&scene=21#wechat_redirect)
  - [github](https://github.com/bojone/gan/)
- [变分自编码器VAE：原来是这么一回事 | 附开源代码 - PaperWeekly的文章 - 知乎](https://zhuanlan.zhihu.com/p/34998569)