<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://ucas.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ucas.io/" rel="alternate" type="text/html" /><updated>2023-08-22T13:26:11+08:00</updated><id>https://ucas.io/feed.xml</id><title type="html">Notes B</title><subtitle>A place for notes on various topics.</subtitle><author><name>麦丽素</name></author><entry><title type="html">WGAN-GP</title><link href="https://ucas.io/ai/WGAN-GP/" rel="alternate" type="text/html" title="WGAN-GP" /><published>2023-08-21T21:05:19+08:00</published><updated>2023-08-21T21:05:19+08:00</updated><id>https://ucas.io/ai/WGAN-GP</id><content type="html" xml:base="https://ucas.io/ai/WGAN-GP/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;h4 id=&quot;wgan-gp-wasserstein-gan-with-gradient-penalty&quot;&gt;WGAN-GP: Wasserstein GAN with Gradient Penalty&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.00028v3&quot;&gt;Paper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;论文要点&quot;&gt;论文要点&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Wasserstein GAN
    &lt;ul&gt;
      &lt;li&gt;WGAN在训练过程中可能遇到不稳定性和收敛困难的问题，而WGAN-GP通过引入梯度惩罚项来替代权重裁剪，能够有效提升GAN的稳定性和训练效果。&lt;/li&gt;
      &lt;li&gt;WGAN-GP中，通过对批次中真实样本和生成样本之间直线上的点进行采样，并对生成器的梯度进行惩罚来实现Lipschitz约束。这种方法能够在不进行任何超参数调整的情况下，稳定地训练各种GAN架构，包括具有101层ResNet的图像生成模型和连续生成器的语言模型。&lt;/li&gt;
      &lt;li&gt;在实验中，通过对CIFAR-10和LSUN卧室数据集进行训练和生成，展示了WGAN-GP相对于传统权重裁剪方法在训练速度和样本质量上的改进。论文还对200个随机架构进行了训练并对比其性能，结果显示WGAN-GP能够成功训练大多数架构。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;需要理解KL散度，JS散度（KL的对称版）&lt;/li&gt;
  &lt;li&gt;GAN
    &lt;ul&gt;
      &lt;li&gt;D(Y, θ)：其中相对熵被用神经网络训练(希望尽可能大-网络可以区分)&lt;/li&gt;
      &lt;li&gt;G(X, θ)：希望两个分布尽可能相似(希望尽可能小)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://kexue.fm/archives/4439&quot;&gt;互怼的艺术：从零直达WGAN-GP&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;table&gt;
          &lt;tbody&gt;
            &lt;tr&gt;
              &lt;td&gt;[公众号：PaperWeekly 第41期&lt;/td&gt;
              &lt;td&gt;互怼的艺术：从零直达 WGAN-GP](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&amp;amp;mid=2247484880&amp;amp;idx=1&amp;amp;sn=4b2e976cc715c9fe2d022ff6923879a8&amp;amp;chksm=96e9da50a19e5346307b54f5ce172e355ccaba890aa157ce50fda68eeaccba6ea05425f6ad76&amp;amp;scene=21#wechat_redirect)&lt;/td&gt;
            &lt;/tr&gt;
          &lt;/tbody&gt;
        &lt;/table&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/bojone/gan/&quot;&gt;github&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[变分自编码器VAE：原来是这么一回事&lt;/td&gt;
          &lt;td&gt;附开源代码 - PaperWeekly的文章 - 知乎](https://zhuanlan.zhihu.com/p/34998569)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><summary type="html">Introduction</summary></entry><entry><title type="html">MeshNet Latent space</title><link href="https://ucas.io/3d/ai/Mesh-Latent-space/" rel="alternate" type="text/html" title="MeshNet Latent space" /><published>2023-08-20T17:14:46+08:00</published><updated>2023-08-20T17:14:46+08:00</updated><id>https://ucas.io/3d/ai/Mesh%20Latent%20space</id><content type="html" xml:base="https://ucas.io/3d/ai/Mesh-Latent-space/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;h4 id=&quot;deepsdf-learning-continuous-signed-distance-functions-for-shape-representation&quot;&gt;DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation&lt;/h4&gt;

&lt;h5 id=&quot;text-to-3d&quot;&gt;text to 3d&lt;/h5&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://3dfy.ai/&quot;&gt;3dfy&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;text to 3d, image to 3d&lt;/li&gt;
      &lt;li&gt;objects
        &lt;ul&gt;
          &lt;li&gt;interiors: table lamps、sofa、table、ottoman、cutlery&lt;/li&gt;
          &lt;li&gt;gaming：sword、shield、axes&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;提供API&lt;/li&gt;
      &lt;li&gt;面像对象：GTA&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://3dfy.ai/technology&quot;&gt;架构和技术方案&lt;/a&gt;(没有具体写参照的论文)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dreamfusion3d.github.io/&quot;&gt;DreamFusion - google&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;平均1.5小时&lt;/li&gt;
      &lt;li&gt;文本-&amp;gt;2D图像-&amp;gt;优化为Nerf
        &lt;ul&gt;
          &lt;li&gt;Score Jacobian Chaining(目前几乎所有的零样本开放域文生 3D 工作所使用的算法)&lt;/li&gt;
          &lt;li&gt;问题：过于平滑、过饱和现象严重&lt;/li&gt;
          &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820201944.png&quot; alt=&quot;20230820201944&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/ashawkey/stable-dreamfusion&quot;&gt;stable-dreamfusion&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;python + cuda&lt;/li&gt;
          &lt;li&gt;
            &lt;blockquote&gt;
              &lt;p&gt;A pytorch implementation of the text-to-3D model Dreamfusion&lt;/p&gt;
            &lt;/blockquote&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/thu-ml/prolificdreamer&quot;&gt;prolificdreamer - 清华 2023&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2305.16213&quot;&gt;Paper&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;Code: release soon&lt;/li&gt;
          &lt;li&gt;DataSet: &lt;a href=&quot;https://arxiv.org/abs/2210.08402&quot;&gt;LAION-5B&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;A dataset consisting of 5.85 billion CLIP-filtered image-text pairs, featuring several nearest neighbor indices, an improved web-interface for exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;VSD(Variational Score Distillation)变分得分蒸馏 + Nerf&lt;/li&gt;
          &lt;li&gt;将3D优化转为2D图像概率分布&lt;/li&gt;
          &lt;li&gt;解决了 DreamFusion 所提 SDS 算法的过饱和、过平滑、缺少多样性等问题&lt;/li&gt;
          &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820201728.png&quot; alt=&quot;20230820201728&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820202131.png&quot; alt=&quot;20230820202131&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://developer.aliyun.com/article/1238277&quot;&gt;开发者社区 机器之心 文章 正文
无需任何3D数据，直接文本生成高质量3D内容，清华朱军团队带来重大进展 - 阿里云&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://research.nvidia.com/labs/dir/magic3d/&quot;&gt;Magic3D - Nvidia&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;平均40分钟&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/amusi1994/article/details/128030083&quot;&gt;一句话生成3D模型！NVIDIA提出Magic3D：高分辨率文本到3D内容创建 - CSDN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;Magic3D提出了一个两阶段的优化框架，通过使用不同分辨率的扩散先验进行粗到精的优化过程。这使得Magic3D能够生成更高分辨率的几何体和纹理。而DreamFusion在这方面存在限制，无法获取高分辨率的几何体和纹理。&lt;/li&gt;
      &lt;li&gt;Magic3D采用了一种多分辨率哈希网格编码架构，使用具有32个隐藏单元的单层MLP来预测RGB颜色、体积密度和法线。同时，Magic3D还引入了密度基础剪枝方法来稀疏表示，并通过密度偏置来优化粗糙神经场表示。&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820210204.png&quot; alt=&quot;20230820210204&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/openai/shap-e&quot;&gt;Shap-E: Generating Conditional 3D Implicit Functions - 2023 - OpenAI&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;dataset：貌似是私有的&lt;/li&gt;
      &lt;li&gt;隐式空间的编码器训练
        &lt;ul&gt;
          &lt;li&gt;点云-&amp;gt;点卷积-&amp;gt;交叉注意力-&amp;gt;patch-&amp;gt;隐式空间的编码表示&lt;/li&gt;
          &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820221546.png&quot; alt=&quot;20230820221546&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Latent Diffusion 条件扩散模型&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.08751&quot;&gt;Point-E: A System for Generating 3D Point Clouds from Complex Prompts - OpenAI&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;text -&amp;gt; point cloud -&amp;gt; mesh(based on sdf)&lt;/li&gt;
      &lt;li&gt;基于文本生成合成视图，基于合成视图生成粗糙的点云，最后根据低分辨率点云和合成视图生成精细的点云。&lt;/li&gt;
      &lt;li&gt;条件扩散模型&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xcfnhrYqKac&quot;&gt;youtube MeshNet++&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/iMoonLab/MeshNet&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="3d" /><category term="AI" /><summary type="html">Introduction</summary></entry><entry><title type="html">MeshNet</title><link href="https://ucas.io/3d/ai/MeshNet-copy/" rel="alternate" type="text/html" title="MeshNet" /><published>2023-08-20T16:11:28+08:00</published><updated>2023-08-20T16:11:28+08:00</updated><id>https://ucas.io/3d/ai/MeshNet%20copy</id><content type="html" xml:base="https://ucas.io/3d/ai/MeshNet-copy/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;h4 id=&quot;deepsdf-learning-continuous-signed-distance-functions-for-shape-representation&quot;&gt;DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation&lt;/h4&gt;

&lt;h5 id=&quot;over-view&quot;&gt;Over View&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820161208.png&quot; alt=&quot;20230820161208&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;数据处理&quot;&gt;数据处理&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820162244.png&quot; alt=&quot;20230820162244&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;面旋转卷积&quot;&gt;面旋转卷积&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820165548.png&quot; alt=&quot;20230820165548&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;面核卷积&quot;&gt;面核卷积&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820165656.png&quot; alt=&quot;20230820165656&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;mesh-convolution&quot;&gt;mesh convolution&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820162309.png&quot; alt=&quot;20230820162309&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据集
    &lt;ul&gt;
      &lt;li&gt;ModelNet40&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;代码
    &lt;ul&gt;
      &lt;li&gt;python&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/iMoonLab/MeshNet&quot;&gt;github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;论文要点&quot;&gt;论文要点&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;将输入转化为空间描述符合结构描述符两种，最后通过mesh 卷积得到全局特征&lt;/li&gt;
  &lt;li&gt;结构描述符包含面核相关性、面旋转卷积
    &lt;ul&gt;
      &lt;li&gt;面核相关性由作者定义的函数进行计算&lt;/li&gt;
      &lt;li&gt;面旋转卷积由名片各点到面中心的向量进行计算&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;空间描述符主要使用mesh中每个平面的几个中心直接进行MLP处理
后续增加池化等，活动全局特征&lt;/li&gt;
  &lt;li&gt;论文不提供重建过程，更多用于分类&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;reference&quot;&gt;Reference&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xcfnhrYqKac&quot;&gt;youtube MeshNet++&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/iMoonLab/MeshNet&quot;&gt;Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="3d" /><category term="AI" /><summary type="html">Introduction</summary></entry><entry><title type="html">DeepSDF</title><link href="https://ucas.io/3d/ai/DeepSDF/" rel="alternate" type="text/html" title="DeepSDF" /><published>2023-08-19T18:04:02+08:00</published><updated>2023-08-19T18:04:02+08:00</updated><id>https://ucas.io/3d/ai/DeepSDF</id><content type="html" xml:base="https://ucas.io/3d/ai/DeepSDF/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;h4 id=&quot;deepsdf-learning-continuous-signed-distance-functions-for-shape-representation&quot;&gt;DeepSDF: Learning Continuous Signed Distance Functions for Shape Representation&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230819180636.png&quot; alt=&quot;20230819180636&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230819194323.png&quot; alt=&quot;20230819194323&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;数据集
    &lt;ul&gt;
      &lt;li&gt;ShapeNet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;代码
    &lt;ul&gt;
      &lt;li&gt;C++\python&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;学习连续的有符号距离函数（Signed Distance Function，SDF）
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Signed_distance_function&quot;&gt;WIKI SDF&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://www.bimant.com/blog/signed-distance-field-implementation/&quot;&gt;有符号距离场原理及实现&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/qq_41368247/article/details/106194092&quot;&gt;2D基本图形的Sign Distance Function (SDF)详解（上）&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SDF的零极线，差不多是为0的等势面&lt;/li&gt;
  &lt;li&gt;输入：
    &lt;ul&gt;
      &lt;li&gt;3D点云 + 噪声或缺失&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;用于训练SDF，并构建3D模型&lt;/li&gt;
  &lt;li&gt;模型通过输入的部分和嘈杂的3D数据点，使用神经网络进行训练，并生成一个能够表示形状表面的连续距离函数。这个距离函数可以通过计算一个点到形状边界的距离，并根据符号指示区域是形状内部还是外部。通过学习这样的连续距离函数，模型能够实现对形状的表示、重建、插值和完成任务。&lt;/li&gt;
  &lt;li&gt;DeepSDF通过神经网络将未知形状的表面表示为SDF的零级线，并可用于插值和完成部分和噪声3D数据。相比之前的方法，DeepSDF在学习的3D形状表示和完成任务上取得了最先进的性能。这项工作的关键创新之处在于引入了自动解码器（auto-decoder）的学习方案，将形状编码和网络参数联合进行优化，同时在数据准备、网络架构和实验设计等方面进行了深入的探索和分析。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;The authors normalized each mesh to a unit sphere and sampled singed distance values for 500,000 spatial points. To capture greater geometric detail of the objects for better model training, sampling was most aggressive near surface areas.
Compared with traditional auto-encoders with both encoder and decoder architectures, &lt;strong&gt;such decoder-only network&lt;/strong&gt;s make the model more compact while maintaining good performance.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;直接用隐变量去Auto-Decoder&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Training a specific neural network for each shape is neither
feasible nor very useful. Instead, we want a model that
can represent a wide variety of shapes, discover their common
properties, and embed them in a low dimensional latent
space. To this end, we introduce a latent vector z, which can
be thought of as encoding the desired shape, as a second input
to the neural network as depicted in Fig. 3b.
&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230820005850.png&quot; alt=&quot;20230820005850&quot; /&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;notice&quot;&gt;Notice&lt;/h2&gt;

&lt;p&gt;git&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Shape Completion
The current release does not include code for shape completion. Please check back later!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/facebookresearch/DeepSDF&quot;&gt;https://github.com/facebookresearch/DeepSDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cameronrwolfe.substack.com/p/3d-generative-modeling-with-deepsdf&quot;&gt;3D Generative Modeling with DeepSDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/3d-generative-modeling-with-deepsdf-2cd06f1ec9b3&quot;&gt;3D Generative Modeling with DeepSDF&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/syncedreview/facebook-mit-uw-introduce-deepsdf-ai-for-3d-shape-representation-75416481e1be&quot;&gt;Facebook, MIT &amp;amp; UW Introduce DeepSDF AI for 3D Shape Representation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=LILRJzMQw5o&amp;amp;t=1s&quot;&gt;Youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=1iuLxJmQII0&quot;&gt;CSC2547 DeepSDF Learning Continuous Signed Distance Functions for Shape Representation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="3d" /><category term="AI" /><summary type="html">Introduction</summary></entry><entry><title type="html">Dreambooth</title><link href="https://ucas.io/ai/Dreambooth/" rel="alternate" type="text/html" title="Dreambooth" /><published>2023-07-08T18:59:17+08:00</published><updated>2023-07-08T18:59:17+08:00</updated><id>https://ucas.io/ai/Dreambooth</id><content type="html" xml:base="https://ucas.io/ai/Dreambooth/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;h4 id=&quot;dreambooth-fine-tuning-text-to-image-diffusion-models-for-subject-driven-generation&quot;&gt;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;It’s like a photo booth, but once the subject is captured, it can be synthesized wherever your dreams take you.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230708190125.png&quot; alt=&quot;20230708190125&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在微调层面，我们重新使用图片进行微调训练会导致几个问题
    &lt;ul&gt;
      &lt;li&gt;语言漂移【学了新的忘了旧的】
        &lt;ul&gt;
          &lt;li&gt;在大模型上微调之后，逐渐失去语言的句法和语义知识。即失去对一般知识的理解&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;过度拟合
        &lt;ul&gt;
          &lt;li&gt;如图所示，在输入狗的照片后，过拟合会导致狗的姿势一直趴在沙滩上，而非其他姿势&lt;/li&gt;
          &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230708205425.png&quot; alt=&quot;20230708205425&quot; /&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dreambooth的优势
    &lt;ul&gt;
      &lt;li&gt;使用了一个新的罕见词来代表图片的含义，保证新加入的图片对应的词在模型中没有其他太多含义&lt;/li&gt;
      &lt;li&gt;区别于Textual inversion方法，Dreambooth使用罕见次，而textual inversion是新词。Dreambooth会对整个模型进行微调，而textual inversion只对embedding部分调整&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dreambooth.github.io/&quot;&gt;dreambooth github&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/616245445&quot;&gt;【AI绘画】LoRA训练与正则化的真相：Dreambooth底层原理 - 秋葉aaaki的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/620577688&quot;&gt;stable diffusion——Dreambooth原理与实践 - 冲弱的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><summary type="html">Introduction</summary></entry><entry><title type="html">Lora</title><link href="https://ucas.io/ai/LORA/" rel="alternate" type="text/html" title="Lora" /><published>2023-07-08T02:24:17+08:00</published><updated>2023-07-08T02:24:17+08:00</updated><id>https://ucas.io/ai/LORA</id><content type="html" xml:base="https://ucas.io/ai/LORA/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;h4 id=&quot;lora-low-rank-adaptation-of-large-language-models&quot;&gt;LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230708022434.png&quot; alt=&quot;20230707020805&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;论文的基本假设，对于一个大模型，参数的秩通常是满的，但是对于一个特定领域的微调模型，存在一个更低维度的秩，使得模型可以适配于特定领域。因此，对于微调的模型，可以通过Lora的方式来训练&lt;/li&gt;
  &lt;li&gt;其基本公式$W=W_0+\Delta W=W_0+BA$&lt;/li&gt;
  &lt;li&gt;其中$B\in\reals^{d\times r},A\in\reals^{r\times k}$&lt;/li&gt;
  &lt;li&gt;秩$r\llless\min(d, k)$&lt;/li&gt;
  &lt;li&gt;对于$h=W_0x+\Delta Wx=W_0x+BAx$&lt;/li&gt;
  &lt;li&gt;只需要适当训练BA即可&lt;/li&gt;
  &lt;li&gt;其中A使用随机高斯初始化，B使用0初始化，因此，训练开始时$BA=0$&lt;/li&gt;
  &lt;li&gt;该算法可以应用到大部分矩阵，例如对于Transformer[QKVO]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/619468521&quot;&gt;LoRA论文回顾 - 我是阿豪啊的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/611557340&quot;&gt;论文阅读：LORA-大型语言模型的低秩适应 - 小虎AI珏爷的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><summary type="html">Introduction</summary></entry><entry><title type="html">Palette: Image-to-Image Diffusion Models</title><link href="https://ucas.io/ai/Image-to-Image-copy/" rel="alternate" type="text/html" title="Palette: Image-to-Image Diffusion Models" /><published>2023-07-07T02:18:17+08:00</published><updated>2023-07-07T02:18:17+08:00</updated><id>https://ucas.io/ai/Image-to-Image%20copy</id><content type="html" xml:base="https://ucas.io/ai/Image-to-Image-copy/">&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;第一篇基于Conditional Diffusion 的 Image to Image 任务&lt;/li&gt;
  &lt;li&gt;在训练Diffusion时，L1， L2的作用，以及self attention
    &lt;ul&gt;
      &lt;li&gt;L1 和 L2的结果质量相似，但L2有更好的多样性&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230707020805.png&quot; alt=&quot;20230707020805&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;损失函数增加了额外的条件信息x
&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230707020620.png&quot; alt=&quot;20230707020620&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230707020103.png&quot; alt=&quot;20230707020103&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[注意力机制的本质&lt;/td&gt;
          &lt;td&gt;Self-Attention&lt;/td&gt;
          &lt;td&gt;Transformer&lt;/td&gt;
          &lt;td&gt;QKV矩阵 - BiliBili](https://www.bilibili.com/video/BV1dt4y1J7ov/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=fd373f40f4a1d2e059be533c5b77797f)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1814y1S7US/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=fd373f40f4a1d2e059be533c5b77797f&quot;&gt;Cross Attention _ Method Explanation _ Math Explained - BiliBili&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><summary type="html">Introduction</summary></entry><entry><title type="html">Self-Attention</title><link href="https://ucas.io/ai/Self-Attention-copy/" rel="alternate" type="text/html" title="Self-Attention" /><published>2023-07-04T22:18:17+08:00</published><updated>2023-07-04T22:18:17+08:00</updated><id>https://ucas.io/ai/Self-Attention%20copy</id><content type="html" xml:base="https://ucas.io/ai/Self-Attention-copy/">&lt;h2 id=&quot;self-attention&quot;&gt;Self-Attention&lt;/h2&gt;

&lt;p&gt;以腰围和身高举例&lt;/p&gt;

&lt;p&gt;腰围是K，身高是V，有对应关系。这时我希望用Q对合适的K进行查询，Q也是腰围，我希望得到合适的V，一般情况下我需要计算各个腰围K的权重并与V相乘，得到最终结果
若KQV矩阵是自身，则是自注意力机制。&lt;/p&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;h3 id=&quot;self-attention-1&quot;&gt;Self Attention&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230704231311.png&quot; alt=&quot;20230704231311&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230704222448.png&quot; alt=&quot;20230704222448&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230704222250.png&quot; alt=&quot;20230704222250&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230704230909.png&quot; alt=&quot;20230704230909&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;cross-attention&quot;&gt;Cross Attention&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230704230959.png&quot; alt=&quot;20230704230959&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230704231126.png&quot; alt=&quot;20230704231126&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[注意力机制的本质&lt;/td&gt;
          &lt;td&gt;Self-Attention&lt;/td&gt;
          &lt;td&gt;Transformer&lt;/td&gt;
          &lt;td&gt;QKV矩阵 - BiliBili](https://www.bilibili.com/video/BV1dt4y1J7ov/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=fd373f40f4a1d2e059be533c5b77797f)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1814y1S7US/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=fd373f40f4a1d2e059be533c5b77797f&quot;&gt;Cross Attention _ Method Explanation _ Math Explained - BiliBili&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><summary type="html">Self-Attention</summary></entry><entry><title type="html">ControlNet</title><link href="https://ucas.io/ai/ControlNet-copy/" rel="alternate" type="text/html" title="ControlNet" /><published>2023-07-03T02:13:07+08:00</published><updated>2023-07-03T02:13:07+08:00</updated><id>https://ucas.io/ai/ControlNet%20copy</id><content type="html" xml:base="https://ucas.io/ai/ControlNet-copy/">&lt;h2 id=&quot;controlnet&quot;&gt;ControlNet&lt;/h2&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;利用Stable Diffusion的Copy，对模型的控制模型进行额外的训练。同时对Stable Diffusion的模型进行冻结，转成对ControlNet进行训练&lt;/li&gt;
  &lt;li&gt;ControlNet的初始化使用Zero Convolution, 初始化为0的卷积核为1的卷积层。&lt;/li&gt;
  &lt;li&gt;对每次的输出运算同步到Stable Diffusion当中&lt;/li&gt;
  &lt;li&gt;实际网络连接
&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230703021632.png&quot; alt=&quot;20230703021632&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;[ ]&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/2302.05543.pdf&quot;&gt;Adding Conditional Control to Text-to-Image Diffusion Models - arxiv&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV13x4y1w7k1/?spm_id_from=333.337.search-card.all.click&amp;amp;vd_source=fd373f40f4a1d2e059be533c5b77797f&quot;&gt;ControlNet - BiliBili&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><summary type="html">ControlNet</summary></entry><entry><title type="html">Diffusion Model</title><link href="https://ucas.io/ai/Diffusion-Model-copy/" rel="alternate" type="text/html" title="Diffusion Model" /><published>2023-07-02T19:05:29+08:00</published><updated>2023-07-02T19:05:29+08:00</updated><id>https://ucas.io/ai/Diffusion-Model%20copy</id><content type="html" xml:base="https://ucas.io/ai/Diffusion-Model-copy/">&lt;h2 id=&quot;diffusion-model&quot;&gt;Diffusion Model&lt;/h2&gt;

&lt;h3 id=&quot;introduction&quot;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;算法主要分为2个部分，第一步讲解关于扩散模型的数据来源和扩散的推导理论，第二部讲解关于反向递推的过程
其中，模型预测的为上一步的噪音值，而非真实值。&lt;/p&gt;

&lt;p&gt;$\alpha_t=1-\beta_t, beta=0.00001 to 0.002, step =200$&lt;/p&gt;

&lt;p&gt;$x_t=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}z_1$&lt;/p&gt;

&lt;p&gt;$x_{t-1}=\sqrt{\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_{t-1}}z_2$&lt;/p&gt;

&lt;p&gt;$x_t=\sqrt{\alpha_t\alpha_{t-1}}x_{t-2}+\sqrt{1-\alpha_t\alpha_{t-1}}\bar{z_2}$&lt;/p&gt;

&lt;p&gt;$x_t=\sqrt{\bar{\alpha_t}}x_0+\sqrt{1-\bar{\alpha_t}}\bar{z_t}$&lt;/p&gt;

&lt;h3 id=&quot;sample&quot;&gt;Sample&lt;/h3&gt;

&lt;p&gt;采样过程即增加噪音的过程，噪音服从正太分布，每一步的权重根据步骤会进行变化，可以理解为，开始加的噪音小，后面加的噪音大。
每一步增加的噪音服从正态分布，因此可以根据给定$x_0$得知任意时刻的$x_t$。将每一步增加的噪音值作为GT，通过$x_t$预测$x_{t-1}$时所增加的噪音值&lt;/p&gt;

&lt;p&gt;权重推导&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230705013847.png&quot; alt=&quot;20230705013847&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230702195416.png&quot; alt=&quot;20230702195416&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20230702195612.png&quot; alt=&quot;20230702195612&quot; /&gt;&lt;/p&gt;

&lt;p&gt;训练过程使用了UNet作为噪音的编解码模型对噪音进行预测&lt;/p&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/cj-express-tech-tildi/stable-diffusion-theory-and-application-a0f98881cb03&quot;&gt;Stable Diffusion: Theory and Applications - medium&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV13h411V7vg?p=2&amp;amp;vd_source=fd373f40f4a1d2e059be533c5b77797f&quot;&gt;Diffusion - BiliBili&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><summary type="html">Diffusion Model</summary></entry></feed>