<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://ucas.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ucas.io/" rel="alternate" type="text/html" /><updated>2024-03-26T14:20:26+08:00</updated><id>https://ucas.io/feed.xml</id><title type="html">Notes B</title><subtitle>A place for notes on various topics.</subtitle><author><name>麦丽素</name></author><entry><title type="html">Sora: The paper you need to read</title><link href="https://ucas.io/ai/3d/Sora/" rel="alternate" type="text/html" title="Sora: The paper you need to read" /><published>2024-02-19T15:37:48+08:00</published><updated>2024-02-19T15:37:48+08:00</updated><id>https://ucas.io/ai/3d/Sora</id><content type="html" xml:base="https://ucas.io/ai/3d/Sora/">&lt;h2 id=&quot;papers&quot;&gt;Papers&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Dall-e 3
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2006.11807&quot;&gt;Improving Image Captioning with Better Use of Captions
&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/682703025&quot;&gt;Dalle-3论文阅读 - nlpcver的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vq-vae
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1711.00937&quot;&gt;Neural Discrete Representation Learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DiT
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2212.09748&quot;&gt;Scalable Diffusion Models with Transformers&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.03429&quot;&gt;Generating Long Videos of Dynamic Scenes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2104.10157&quot;&gt;VideoGPT: Video Generation using VQ-VAE and Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2210.02303&quot;&gt;Imagen Video: High Definition Video Generation with Diffusion Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2304.08818&quot;&gt;Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.06662&quot;&gt;Photorealistic Video Generation with Diffusion Models&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2103.15691&quot;&gt;ViViT: A Video Vision Transformer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;长上下文训练
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2402.08268&quot;&gt;World Model on Million-Length Video And Language With RingAttention&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.14509&quot;&gt;DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2401.12945&quot;&gt;Lumiere: A Space-Time Diffusion Model for Video Generation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/lucidrains/magvit2-pytorch&quot;&gt;magvit2&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;video vae encoder-decoder&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.06304&quot;&gt;Patch n’ Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2206.10789&quot;&gt;Scaling Autoregressive Models for Content-Rich Text-to-Image Generation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2203.12602&quot;&gt;VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2312.14125&quot;&gt;VideoPoet: A Large Language Model for Zero-Shot Video Generation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;分辨率上不去&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">Papers</summary></entry><entry><title type="html">SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models</title><link href="https://ucas.io/ai/3d/SparseCtrl/" rel="alternate" type="text/html" title="SparseCtrl: Adding Sparse Controls to Text-to-Video Diffusion Models" /><published>2024-02-09T18:57:13+08:00</published><updated>2024-02-09T18:57:13+08:00</updated><id>https://ucas.io/ai/3d/SparseCtrl</id><content type="html" xml:base="https://ucas.io/ai/3d/SparseCtrl/">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2311.16933&quot;&gt;SparseCtrl&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240209185739.png&quot; alt=&quot;20240209185739&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;论文主要验证了稀疏控制在text2video方面的作用&lt;/li&gt;
      &lt;li&gt;原本的条件控制主要在稠密的视频流生成中起作用，但通过稀疏信号的控制一样可以得到较好的结果
        &lt;ul&gt;
          &lt;li&gt;该方法在预训练的T2V模型之上增加了一个附加的条件编码器&lt;/li&gt;
          &lt;li&gt;通过在帧间复制ControlNet编码器，并将条件图像添加到帧中，实现了条件信号在帧间的传播，以增强时间一致性&lt;/li&gt;
          &lt;li&gt;通过计算生成视频中的关键帧的深度图像与原始视频中提取的深度图像之间的mse进行评估视频的事件一致性&lt;/li&gt;
          &lt;li&gt;WebVid-10M数据集&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.06721&quot;&gt;IpAdaptor&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240209194938.png&quot; alt=&quot;20240209194938&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;IP-Adapter通过解耦的跨注意力策略，将图像特征嵌入到预训练的文本到图像扩散模型中实现图像提示能力&lt;/li&gt;
      &lt;li&gt;将图像特征嵌入到预训练的文本到图像扩散模型中。IP-Adapter由图像编码器和带有解耦跨注意力的调整模块组成&lt;/li&gt;
      &lt;li&gt;两个开源数据集（LAION-2B和COYO-700M）的大约1000万个文本-图像对&lt;/li&gt;
      &lt;li&gt;IP-Adapter由图像编码器和带有解耦跨注意力的调整模块组成。图像编码器使用预训练的CLIP图像编码器从图像提示中提取图像特征。解耦的跨注意力策略通过在原始UNet模型中添加新的跨注意力层来嵌入图像特征，从而实现更有效的图像提示适配器。&lt;/li&gt;
      &lt;li&gt;使用一个大规模的多模态数据集进行IP-Adapter的训练，其中包含来自不同数据集的文本-图像对。训练过程使用AdamW优化器进行，使用DeepSpeed ZeRO-2加速训练。进行了定量和定性的实验评估，比较IP-Adapter与其他适配器方法的性能和效果。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://daveredrum.github.io/Text2Tex/&quot;&gt;Text2Tex&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240209203142.png&quot; alt=&quot;20240209203142&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;从多个视点逐步合成高分辨率的部分纹理。为了避免在视角变化过程中累积不一致和拉伸的伪影，作者将渲染的视图动态分割为生成掩模，表示每个可见像素的生成状态&lt;/li&gt;
      &lt;li&gt;为了控制扩散过程中的随机性，引入了一个缩放因子γ，通过控制扩散步骤的数量来开始去噪潜在编码&lt;/li&gt;
      &lt;li&gt;使用修复掩模指导采样过程&lt;/li&gt;
      &lt;li&gt;通过视角投射和插值，减少纹理的伪影和拉伸问题&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">SparseCtrl 论文主要验证了稀疏控制在text2video方面的作用 原本的条件控制主要在稠密的视频流生成中起作用，但通过稀疏信号的控制一样可以得到较好的结果 该方法在预训练的T2V模型之上增加了一个附加的条件编码器 通过在帧间复制ControlNet编码器，并将条件图像添加到帧中，实现了条件信号在帧间的传播，以增强时间一致性 通过计算生成视频中的关键帧的深度图像与原始视频中提取的深度图像之间的mse进行评估视频的事件一致性 WebVid-10M数据集 IpAdaptor IP-Adapter通过解耦的跨注意力策略，将图像特征嵌入到预训练的文本到图像扩散模型中实现图像提示能力 将图像特征嵌入到预训练的文本到图像扩散模型中。IP-Adapter由图像编码器和带有解耦跨注意力的调整模块组成 两个开源数据集（LAION-2B和COYO-700M）的大约1000万个文本-图像对 IP-Adapter由图像编码器和带有解耦跨注意力的调整模块组成。图像编码器使用预训练的CLIP图像编码器从图像提示中提取图像特征。解耦的跨注意力策略通过在原始UNet模型中添加新的跨注意力层来嵌入图像特征，从而实现更有效的图像提示适配器。 使用一个大规模的多模态数据集进行IP-Adapter的训练，其中包含来自不同数据集的文本-图像对。训练过程使用AdamW优化器进行，使用DeepSpeed ZeRO-2加速训练。进行了定量和定性的实验评估，比较IP-Adapter与其他适配器方法的性能和效果。 Text2Tex 从多个视点逐步合成高分辨率的部分纹理。为了避免在视角变化过程中累积不一致和拉伸的伪影，作者将渲染的视图动态分割为生成掩模，表示每个可见像素的生成状态 为了控制扩散过程中的随机性，引入了一个缩放因子γ，通过控制扩散步骤的数量来开始去噪潜在编码 使用修复掩模指导采样过程 通过视角投射和插值，减少纹理的伪影和拉伸问题</summary></entry><entry><title type="html">AnimeDiff</title><link href="https://ucas.io/ai/3d/Deep-Image-Blend-copy/" rel="alternate" type="text/html" title="AnimeDiff" /><published>2024-02-09T18:40:45+08:00</published><updated>2024-02-09T18:40:45+08:00</updated><id>https://ucas.io/ai/3d/Deep%20Image%20Blend%20copy</id><content type="html" xml:base="https://ucas.io/ai/3d/Deep-Image-Blend-copy/">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2307.04725&quot;&gt;AnimeDiff&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240209184118.png&quot; alt=&quot;20240209184139&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240209184139.png&quot; alt=&quot;20240209184139&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;论文主要使用stablediffusion中增加animediff模块进行动画学习
        &lt;ul&gt;
          &lt;li&gt;AnimeDiff包含3个模块
            &lt;ul&gt;
              &lt;li&gt;domain adapter
                &lt;ul&gt;
                  &lt;li&gt;用于减轻基准T2I与视频培训数据之间的视觉分布差异&lt;/li&gt;
                  &lt;li&gt;主要是由于不同视频之间的风格差异导致的&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;运动模块
                &lt;ul&gt;
                  &lt;li&gt;用于学习动画的运动先验&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;MotionLoRA
                &lt;ul&gt;
                  &lt;li&gt;通过针对上一帧的运动模块，进行运动的预测&lt;/li&gt;
                  &lt;li&gt;数据集主要是一些带有运动信息的视频&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">AnimeDiff 论文主要使用stablediffusion中增加animediff模块进行动画学习 AnimeDiff包含3个模块 domain adapter 用于减轻基准T2I与视频培训数据之间的视觉分布差异 主要是由于不同视频之间的风格差异导致的 运动模块 用于学习动画的运动先验 MotionLoRA 通过针对上一帧的运动模块，进行运动的预测 数据集主要是一些带有运动信息的视频</summary></entry><entry><title type="html">Deep Image Blend</title><link href="https://ucas.io/ai/3d/AnimeDiff/" rel="alternate" type="text/html" title="Deep Image Blend" /><published>2024-02-09T18:07:54+08:00</published><updated>2024-02-09T18:07:54+08:00</updated><id>https://ucas.io/ai/3d/AnimeDiff</id><content type="html" xml:base="https://ucas.io/ai/3d/AnimeDiff/">&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/owenzlz/DeepImageBlending&quot;&gt;Deep Image Blend&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20240209180729.png&quot; alt=&quot;20240209180729&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;针对泊松融合的局限性，作者提出了Deep Image Blend
        &lt;ul&gt;
          &lt;li&gt;泊松融合主要是参考融合边界处的像素信息，进行梯度最小的优化融合算法。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;本论文主要联合优化了3种不同的损失
        &lt;ul&gt;
          &lt;li&gt;泊松融合损失、风格损失和内容损失&lt;/li&gt;
          &lt;li&gt;风格损失合内容损失主要引入VGG的风格化向量&lt;/li&gt;
          &lt;li&gt;通过迭代的方法保证了边界的过度&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;论文还涉及了正则化损失，包括直方图损失和总变差损失，以提高图像的平滑度和稳定风格转换。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">Deep Image Blend 针对泊松融合的局限性，作者提出了Deep Image Blend 泊松融合主要是参考融合边界处的像素信息，进行梯度最小的优化融合算法。 本论文主要联合优化了3种不同的损失 泊松融合损失、风格损失和内容损失 风格损失合内容损失主要引入VGG的风格化向量 通过迭代的方法保证了边界的过度 论文还涉及了正则化损失，包括直方图损失和总变差损失，以提高图像的平滑度和稳定风格转换。</summary></entry><entry><title type="html">pytorch 扩展 安装</title><link href="https://ucas.io/ai/3d/pytorch-%E6%89%A9%E5%B1%95-%E5%AE%89%E8%A3%85/" rel="alternate" type="text/html" title="pytorch 扩展 安装" /><published>2023-12-30T23:19:05+08:00</published><updated>2023-12-30T23:19:05+08:00</updated><id>https://ucas.io/ai/3d/pytorch%20%E6%89%A9%E5%B1%95%20%E5%AE%89%E8%A3%85</id><content type="html" xml:base="https://ucas.io/ai/3d/pytorch-%E6%89%A9%E5%B1%95-%E5%AE%89%E8%A3%85/">&lt;h2 id=&quot;涉及项目&quot;&gt;涉及项目&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/xxlong0/Wonder3D&quot;&gt;Wonder3d&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fatal error: filesystem: No such file or directory&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;解决方案，升级gcc到8.0以上&lt;/li&gt;
          &lt;li&gt;参考
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_44128857/article/details/108554751&quot;&gt;Ubuntu问题——Ubuntu18.04 更新gcc和g++的版本号 - CSDN&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;https://github.com/NVlabs/tiny-cuda-nn/issues/337#issue-1800158950&quot;&gt;Failed to build tinycudann - git&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;https://askubuntu.com/a/1028656&quot;&gt;Install gcc-8 only on Ubuntu 18.04? - askubuntu&lt;/a&gt;
                &lt;ul&gt;
                  &lt;li&gt;解决gcc-8和gcc7共存问题
                    &lt;ul&gt;
                      &lt;li&gt;安装高版本[&amp;gt;10]的gcc需要ppa的镜像站, 无国内镜像站
                        &lt;ul&gt;
                          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt install software-properties-common&lt;/code&gt;&lt;/li&gt;
                          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add-apt-repository ppa:ubuntu-toolchain-r/test&lt;/code&gt;&lt;/li&gt;
                          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt update&lt;/code&gt;&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;安装gcc-8: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt install gcc-8 g++-8&lt;/code&gt;&lt;/li&gt;
                  &lt;li&gt;安装gcc-7: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt install gcc-7 g++-7&lt;/code&gt;&lt;/li&gt;
                  &lt;li&gt;配置gcc
                    &lt;ul&gt;
                      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 700 --slave /usr/bin/g++ g++ /usr/bin/g++-7&lt;/code&gt;&lt;/li&gt;
                      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 800 --slave /usr/bin/g++ g++ /usr/bin/g++-8&lt;/code&gt;
                        &lt;ul&gt;
                          &lt;li&gt;中间的数字为优先级，在使用&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --config&lt;/code&gt;时生效&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;选择默认gcc:&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --config gcc&lt;/code&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">涉及项目</summary></entry><entry><title type="html">pytorch 扩展 安装</title><link href="https://ucas.io/ai/3d/pytorch-%E6%89%A9%E5%B1%95-%E5%AE%89%E8%A3%85/" rel="alternate" type="text/html" title="pytorch 扩展 安装" /><published>2023-11-27T16:03:35+08:00</published><updated>2023-11-27T16:03:35+08:00</updated><id>https://ucas.io/ai/3d/pytorch%20%E6%89%A9%E5%B1%95%20%E5%AE%89%E8%A3%85</id><content type="html" xml:base="https://ucas.io/ai/3d/pytorch-%E6%89%A9%E5%B1%95-%E5%AE%89%E8%A3%85/">&lt;h2 id=&quot;涉及项目&quot;&gt;涉及项目&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/xxlong0/Wonder3D&quot;&gt;Wonder3d&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;fatal error: filesystem: No such file or directory&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;解决方案，升级gcc到8.0以上&lt;/li&gt;
          &lt;li&gt;参考
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/weixin_44128857/article/details/108554751&quot;&gt;Ubuntu问题——Ubuntu18.04 更新gcc和g++的版本号 - CSDN&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;https://github.com/NVlabs/tiny-cuda-nn/issues/337#issue-1800158950&quot;&gt;Failed to build tinycudann - git&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;https://askubuntu.com/a/1028656&quot;&gt;Install gcc-8 only on Ubuntu 18.04? - askubuntu&lt;/a&gt;
                &lt;ul&gt;
                  &lt;li&gt;解决gcc-8和gcc7共存问题
                    &lt;ul&gt;
                      &lt;li&gt;安装高版本[&amp;gt;10]的gcc需要ppa的镜像站, 无国内镜像站
                        &lt;ul&gt;
                          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt install software-properties-common&lt;/code&gt;&lt;/li&gt;
                          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;add-apt-repository ppa:ubuntu-toolchain-r/test&lt;/code&gt;&lt;/li&gt;
                          &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt update&lt;/code&gt;&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;安装gcc-8: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt install gcc-8 g++-8&lt;/code&gt;&lt;/li&gt;
                  &lt;li&gt;安装gcc-7: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apt install gcc-7 g++-7&lt;/code&gt;&lt;/li&gt;
                  &lt;li&gt;配置gcc
                    &lt;ul&gt;
                      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-7 700 --slave /usr/bin/g++ g++ /usr/bin/g++-7&lt;/code&gt;&lt;/li&gt;
                      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 800 --slave /usr/bin/g++ g++ /usr/bin/g++-8&lt;/code&gt;
                        &lt;ul&gt;
                          &lt;li&gt;中间的数字为优先级，在使用&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --config&lt;/code&gt;时生效&lt;/li&gt;
                        &lt;/ul&gt;
                      &lt;/li&gt;
                    &lt;/ul&gt;
                  &lt;/li&gt;
                  &lt;li&gt;选择默认gcc:&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;update-alternatives --config gcc&lt;/code&gt;&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">涉及项目</summary></entry><entry><title type="html">3d 生成 商业版本</title><link href="https://ucas.io/ai/3d/3d-%E7%94%9F%E6%88%90-%E5%95%86%E4%B8%9A%E7%89%88/" rel="alternate" type="text/html" title="3d 生成 商业版本" /><published>2023-11-22T10:52:22+08:00</published><updated>2023-11-22T10:52:22+08:00</updated><id>https://ucas.io/ai/3d/3d%20%E7%94%9F%E6%88%90%20%E5%95%86%E4%B8%9A%E7%89%88</id><content type="html" xml:base="https://ucas.io/ai/3d/3d-%E7%94%9F%E6%88%90-%E5%95%86%E4%B8%9A%E7%89%88/">&lt;h2 id=&quot;3d-mesh-生成&quot;&gt;3D Mesh 生成&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;web&lt;/th&gt;
      &lt;th&gt;pipeline&lt;/th&gt;
      &lt;th&gt;price&lt;/th&gt;
      &lt;th&gt;result&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;labs.masterpiecex.com&lt;/td&gt;
      &lt;td&gt;text-&amp;gt;3d&lt;/td&gt;
      &lt;td&gt;3000c/36.99$/month, pic/50c&lt;/td&gt;
      &lt;td&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231122110309.png&quot; alt=&quot;20231122110309&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;g3d.ai&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;waitlist&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ponzu.gg&lt;/td&gt;
      &lt;td&gt;TypeError: Failed to fetch&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;https://www.prometheanai.com/&lt;/td&gt;
      &lt;td&gt;Error&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;leonardo.ai&lt;/td&gt;
      &lt;td&gt;depth-&amp;gt;texture&lt;/td&gt;
      &lt;td&gt;6000t/month 48$/month&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;mirageml.com&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;waitlist&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pixela.ai&lt;/td&gt;
      &lt;td&gt;stable diffusion&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;https://github.com/nv-tlabs/GET3D&lt;/td&gt;
      &lt;td&gt;**&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;www.kaedim3d.com&lt;/td&gt;
      &lt;td&gt;image-&amp;gt;3d&lt;/td&gt;
      &lt;td&gt;60c/1000$/month&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;https://www.kinetix.tech&lt;/td&gt;
      &lt;td&gt;animate&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;https://withpoly.com&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;waitlist&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;https://www.deepmotion.com/&lt;/td&gt;
      &lt;td&gt;animate&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;scenario.com&lt;/td&gt;
      &lt;td&gt;stable diffusion&lt;/td&gt;
      &lt;td&gt;199$/month&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;lumalabs.ai&lt;/td&gt;
      &lt;td&gt;video-3d&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">3D Mesh 生成</summary></entry><entry><title type="html">3d Mesh 深度相关</title><link href="https://ucas.io/ai/3d/3d-Mesh-%E6%B7%B1%E5%BA%A6%E7%9B%B8%E5%85%B3/" rel="alternate" type="text/html" title="3d Mesh 深度相关" /><published>2023-11-12T15:05:17+08:00</published><updated>2023-11-12T15:05:17+08:00</updated><id>https://ucas.io/ai/3d/3d%20Mesh%20%E6%B7%B1%E5%BA%A6%E7%9B%B8%E5%85%B3</id><content type="html" xml:base="https://ucas.io/ai/3d/3d-Mesh-%E6%B7%B1%E5%BA%A6%E7%9B%B8%E5%85%B3/">&lt;h2 id=&quot;3d-mesh-深度学习&quot;&gt;3D Mesh 深度学习&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2106.02285&quot;&gt;Subdivision-Based Mesh Convolution Networks - TOG 2021&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231112150600.png&quot; alt=&quot;20231112155640&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;通过细分网格，获得mesh网络的卷积关键面，将细分后的网格作为输入，进行卷积核池化运算，卷积运算即通过面榻缩的方式或者边榻缩的方式，将榻缩后的面作为卷积输出，同时，池化操作也很类似。&lt;/li&gt;
      &lt;li&gt;作者还给出了不同strip参数和不同kernel参数做卷积运算时与原2d conv的类比图&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231112155640.png&quot; alt=&quot;20231112155640&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;同时论文介绍了基于该思想的不同上采样方法&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3d-mesh-style-transfer&quot;&gt;3D Mesh style transfer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S1524070323000280&quot;&gt;Neural Style Transfer for 3D Meshes&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231112160950.png&quot; alt=&quot;20231112160950&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;是一种基于mesh conv net的3d风格迁移方法&lt;/li&gt;
      &lt;li&gt;网络输入还是mesh的三角面片&lt;/li&gt;
      &lt;li&gt;网路架构类似2d图片风格迁移，可以参考2016年李飞飞的风格迁移论文&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231112162850.png&quot; alt=&quot;20231112162850&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;通过比较特征值之间的损失即可得到，同时，损失包含Content loss和style Loss, 在李飞飞2016的论文当中，同样存在两种loss&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">3D Mesh 深度学习</summary></entry><entry><title type="html">3d Mesh 生成</title><link href="https://ucas.io/ai/3d/3d-Mesh-%E7%94%9F%E6%88%90/" rel="alternate" type="text/html" title="3d Mesh 生成" /><published>2023-10-13T15:25:57+08:00</published><updated>2023-10-13T15:25:57+08:00</updated><id>https://ucas.io/ai/3d/3d%20Mesh%20%E7%94%9F%E6%88%90</id><content type="html" xml:base="https://ucas.io/ai/3d/3d-Mesh-%E7%94%9F%E6%88%90/">&lt;h2 id=&quot;3d-mesh-生成&quot;&gt;3D Mesh 生成&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/p4vv37/NeuralNetworksSketchbook.git&quot;&gt;NeuralNetworksSketchbook - github&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;stable diffusion&lt;/li&gt;
      &lt;li&gt;blender&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img1698393912605.png&quot; alt=&quot;1698393912605&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;格栅化反传&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/TEXTurePaper/TEXTurePaper&quot;&gt;TEXTure: Text-Guided Texturing of 3D Shapes&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231027175344.png&quot; alt=&quot;20231027175344&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;iteration[“keep”, “refine”, “generate”]&lt;/li&gt;
      &lt;li&gt;多较多球面参数化，获得高清纹理&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231027183619.png&quot; alt=&quot;20231027183619&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.17843&quot;&gt;Magic123 - One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;采用两阶段的3dmesh生成方法
        &lt;ul&gt;
          &lt;li&gt;第一阶段使用nerf生成粗糙的3d mesh表示&lt;/li&gt;
          &lt;li&gt;第二阶段用可微的神经网络对粗糙的3d mesh表示进行精细化&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;引入了2D和3D扩散先验的结合&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231112184946.png&quot; alt=&quot;20231112184946&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;论文中提到了姿势化，姿势化是指目标物体或人体在图像中的姿势和表情。在一个单一的未指定姿势的图像中，姿势化是指通过计算机视觉技术和深度学习模型，从图像中推断出物体或人体的三维姿势和形状&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.xxlong.site/Wonder3D/&quot;&gt;Wonder3D: Single Image to 3D using Cross-Domain Diffusion&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231112190551.png&quot; alt=&quot;20231112190551&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;目前的3d生成任务，在面临多头一致性问题时，不管是边缝还是patch融合，作为一种后处理的方式，在基于nerf的求解框架下无法得到很好的解决，因此有很多基于2步的3d mesh和纹理生成任务的工作，但是这些工作都没有从根本上解决该问题，本论文提出了一种基于法向的mesh生成方案，在传统的mesh生成任务中，多头任务都是基于2d图片的，在2d图片中，法向信息被丢失，因此本文提出的wonder3d利用sd的扩散功能在生成2d images时，一并生成了对应角度的法向信息。同时，作者还引入了一种新颖的法线融合算法，可以从多视图的表示中提取出高质量的表面几何。&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231112191602.png&quot; alt=&quot;20231112191602&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2303.11328&quot;&gt;Zero123: Zero-shot One Image to 3D Object&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231230233016.png&quot; alt=&quot;20231230233016&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;使用合成数据集训练一个具有先验知识的大模型，用于对单张图片进行多视角的预测
        &lt;ul&gt;
          &lt;li&gt;数据合成使用blender进行渲染&lt;/li&gt;
          &lt;li&gt;论文使用图片的相对视角进行合成&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;训练方法
        &lt;ul&gt;
          &lt;li&gt;算法目标，计算出合成图片$\hat{x}_{R,T}=f(x,R,T)$&lt;/li&gt;
          &lt;li&gt;训练输入，${(x, x_{(R,T),R,T})}$&lt;/li&gt;
          &lt;li&gt;其中，引入$c(x, R,T)$进行embedding：High-resolution image synthesis
with latent diffusion models&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;算法管道: txt-&amp;gt;img-&amp;gt;3d&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2310.15110&quot;&gt;Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231231022408.png&quot; alt=&quot;20231231022408&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;使用多视图的联合分布，而不是单张图片，以进行更加完整的一致性&lt;/li&gt;
      &lt;li&gt;改进了Condition，使用线性噪声表&lt;/li&gt;
      &lt;li&gt;利用Depth control net进行控制(好像不是特别明显的贡献)&lt;/li&gt;
      &lt;li&gt;主要对比了 zero123, zero123 xl, SyncDreamer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2309.03453&quot;&gt;SyncDreamer: Generating Multiview-consistent Images from a Single-view Image&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231231030127.png&quot; alt=&quot;20231231030127&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;数据集
        &lt;ul&gt;
          &lt;li&gt;Google Scan Dataset&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;主要框架
        &lt;ul&gt;
          &lt;li&gt;使用目标视角进行通过unet训练，作者弄了个共享噪声预测器用于共享信息&lt;/li&gt;
          &lt;li&gt;在图像生成方面，主要使用了zero123的生成能力，上图黄色部分为zero的模型&lt;/li&gt;
          &lt;li&gt;论文好像水了一部分对比实验，在后文中，使用初始的stable diffusion进行对比，说效果比zero123差，额。。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2306.16928&quot;&gt;One-2-3-45: Any Single Image to 3D Mesh in 45 Seconds without Per-Shape Optimization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231231032857.png&quot; alt=&quot;20231231032857&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;45秒左右将单个图像转换为高质量的360度纹理网格，无需进行形状优化&lt;/li&gt;
      &lt;li&gt;几何重建通过引入了SDF信息，进行几何估计，主要用到了相机视角加图片的方法进行体素估计，再转化为SDF，最后进行拍照比对&lt;/li&gt;
      &lt;li&gt;可以无缝地扩展到支持文本到3D的任务，为文本到网格生成等应用提供了可行的解决方案。&lt;/li&gt;
      &lt;li&gt;图片生成也主要依赖于zero123
        &lt;ul&gt;
          &lt;li&gt;其中，对elevation的估计进行了优化&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2308.16512&quot;&gt;MVDREAM: MULTI-VIEW DIFFUSION FOR 3D GENERATION&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img20231231034409.png&quot; alt=&quot;20231231034409&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;灵感：视频-&amp;gt;3d&lt;/li&gt;
      &lt;li&gt;改进了现有的Score Distillation Sampling（SDS）方法
        &lt;ul&gt;
          &lt;li&gt;SDS方法首先使用多视角扩散模型生成一组候选图像。然后，通过计算每个候选图像的分数，选择与输入文本条件最相匹配的候选图像&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;评分蒸馏采样&lt;/strong&gt; （SDS），它在参数空间而不是像素空间中进行采样
            &lt;ul&gt;
              &lt;li&gt;ProlificDream使用变分评分蒸馏(Variational Score Distillation，VSD)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;数据集&quot;&gt;数据集&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;管道
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://gitee.com/Lili_Cheng/DeepSnakes/tree/master/generage_pipes_mesh&quot;&gt;DeepSnakes - gitee&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reference&quot;&gt;Reference&lt;/h2&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">3D Mesh 生成</summary></entry><entry><title type="html">参数化</title><link href="https://ucas.io/ai/3d/%E5%8F%82%E6%95%B0%E5%8C%96/" rel="alternate" type="text/html" title="参数化" /><published>2023-09-28T15:47:38+08:00</published><updated>2023-09-28T15:47:38+08:00</updated><id>https://ucas.io/ai/3d/%E5%8F%82%E6%95%B0%E5%8C%96</id><content type="html" xml:base="https://ucas.io/ai/3d/%E5%8F%82%E6%95%B0%E5%8C%96/">&lt;h2 id=&quot;参数化-parameterization&quot;&gt;参数化 Parameterization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/255169637&quot;&gt;网格参数化原理 - 1： Metric Distortion - Allan的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;比较经典&lt;/li&gt;
      &lt;li&gt;【相容性网络】Compatible mesh
        &lt;ul&gt;
          &lt;li&gt;两个mesh的一一对应关系&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;球面参数化 Spherical parameterization
        &lt;ul&gt;
          &lt;li&gt;球面参数化度量&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;体参数化 Volumetric parameterization
        &lt;ul&gt;
          &lt;li&gt;四面体&lt;/li&gt;
          &lt;li&gt;$\sigma_1\ge\sigma_2\ge\sigma_3$&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;几何映射 (Geometric Mapping)
    &lt;ul&gt;
      &lt;li&gt;$R-&amp;gt;R$ 函数对应&lt;/li&gt;
      &lt;li&gt;$R^2\rightarrow R^2$&lt;/li&gt;
      &lt;li&gt;$M\rightarrow R^2$&lt;/li&gt;
      &lt;li&gt;$M\rightarrow S^2$&lt;/li&gt;
      &lt;li&gt;$M\rightarrow M’$&lt;/li&gt;
      &lt;li&gt;$R^3\rightarrow R^3$&lt;/li&gt;
      &lt;li&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/imgd04569fbaac510add8cd8a9b507021e.jpg&quot; alt=&quot;d04569fbaac510add8cd8a9b507021e&quot; /&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;低维嵌入
    &lt;ul&gt;
      &lt;li&gt;高纬数据可能存在低维结构&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV18T411P7hT/?share_source=copy_web&quot;&gt;【GAMES301-曲面参数化】&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;雅克比矩阵，用于衡量曲面扭曲情况&lt;/li&gt;
      &lt;li&gt;参数化方法
        &lt;ul&gt;
          &lt;li&gt;线性方法
            &lt;ul&gt;
              &lt;li&gt;Tutte 1963; Floater 1997
                &lt;ul&gt;
                  &lt;li&gt;变形&lt;/li&gt;
                  &lt;li&gt;Euclidean-orbifold Aigerman et. al. 2015&lt;/li&gt;
                  &lt;li&gt;Hyperbolic-orbifold Aigerman et. al. 2016&lt;/li&gt;
                  &lt;li&gt;Spherical-orbifold Aigerman et. al. 2017&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;将mesh的边界映射到凸二维图形中，通过线性方法(求解器)，得到一个一定不会翻转的参数化方法&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;但是会产生高扭曲&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;优化方法
            &lt;ul&gt;
              &lt;li&gt;都会产生翻转，因此需要后处理，一步步让翻转消失&lt;/li&gt;
              &lt;li&gt;As-rigid-as-possible(ARAP) Liu et al. 2008 &lt;strong&gt;刘利刚&lt;/strong&gt;
                &lt;ul&gt;
                  &lt;li&gt;CGAL包含&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;ABF/ABF++ Sheffer et al. 2005
                &lt;ul&gt;
                  &lt;li&gt;保角&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
              &lt;li&gt;Simplex Assembly Fu and Liu 2016 2016 &lt;strong&gt;刘利刚&lt;/strong&gt;
                &lt;ul&gt;
                  &lt;li&gt;保角&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;保证无翻转的参数化优化
            &lt;ul&gt;
              &lt;li&gt;先保证无翻转，再优化&lt;/li&gt;
              &lt;li&gt;形变量度量
                &lt;ul&gt;
                  &lt;li&gt;
                    &lt;table&gt;
                      &lt;thead&gt;
                        &lt;tr&gt;
                          &lt;th&gt;方法&lt;/th&gt;
                          &lt;th&gt;Conformal&lt;/th&gt;
                          &lt;th&gt;Maximal Isometric Distortion&lt;/th&gt;
                          &lt;th&gt;MIPS&lt;/th&gt;
                          &lt;th&gt;isometric&lt;/th&gt;
                          &lt;th&gt;Symmetric Dirichlet energy&lt;/th&gt;
                        &lt;/tr&gt;
                      &lt;/thead&gt;
                      &lt;tbody&gt;
                        &lt;tr&gt;
                          &lt;td&gt;年份&lt;/td&gt;
                          &lt;td&gt;Degener et al. 2003&lt;/td&gt;
                          &lt;td&gt;Sorikine et al. 2002&lt;/td&gt;
                          &lt;td&gt;Hormann and Greiner 2000&lt;/td&gt;
                          &lt;td&gt;Aigermann et al. 2014&lt;/td&gt;
                          &lt;td&gt;Smith and Schaefer 2015&lt;/td&gt;
                        &lt;/tr&gt;
                        &lt;tr&gt;
                          &lt;td&gt;度量&lt;/td&gt;
                          &lt;td&gt;$\frac{\sigma_2}{\color{red}{\sigma_1}}$&lt;/td&gt;
                          &lt;td&gt;$\max(\sigma_2, \frac{1}{\color{red}{\sigma_1}})$&lt;/td&gt;
                          &lt;td&gt;$\frac{\color{red}{\sigma_1}}{\sigma_2} + \frac{\sigma_2}{\color{red}{\sigma_1}}$&lt;/td&gt;
                          &lt;td&gt;$\sqrt{\sigma_2^2+\frac{1}{\color{red}{\sigma_1^2}}}$&lt;/td&gt;
                          &lt;td&gt;$\sigma_2^2+\frac{1}{\color{red}{\sigma_1^2}}+{\color{red}{\sigma_1^2}}+\frac{1}{\sigma_2^2}$&lt;/td&gt;
                        &lt;/tr&gt;
                      &lt;/tbody&gt;
                    &lt;/table&gt;
                  &lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/89934333&quot;&gt;微分几何笔记(2) —— 曲线的参数化 - Silence的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;曲线微分&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/89975877&quot;&gt;微分几何笔记(4) —— 二维三维空间中曲线的曲率以及环绕数 - Silence的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;基函数拟合&quot;&gt;基函数拟合&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/FavorMylikes/hackmd-note/img/img1696341320569.jpg&quot; alt=&quot;1696341320569&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://juejin.cn/post/7089391423362957325&quot;&gt;图形学基础——傅里叶变换及球谐函数&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/351289217&quot;&gt;球谐函数介绍（Spherical Harmonics） - 浦夜的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/655283202&quot;&gt;球谐函数原理 - 爅一点都不黑的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;球谐函数推导&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https%3A//ulyanadupletsa.github.io/SphericalCoordinates.pdf&quot;&gt;Laplacian in Spherical Coordinates.pdf&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://2uv.xyz/post/maths/sh/#-%E7%90%83%E8%B0%90%E5%87%BD%E6%95%B0&quot;&gt;渲染中的球谐函数及Deringing - Far Land&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;比较全面🤙&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/401892655&quot;&gt;球谐光照 从入门到放弃 - 王俊杰 hybrid的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;没看懂&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;table&gt;
      &lt;tbody&gt;
        &lt;tr&gt;
          &lt;td&gt;[Non Uniform DFT&lt;/td&gt;
          &lt;td&gt;非均匀离散傅里叶变换&lt;/td&gt;
          &lt;td&gt;NDFT # 介绍 - 向阳的文章 - 知乎](https://zhuanlan.zhihu.com/p/348240324)&lt;/td&gt;
        &lt;/tr&gt;
      &lt;/tbody&gt;
    &lt;/table&gt;
    &lt;ul&gt;
      &lt;li&gt;非均匀傅里叶&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/423540067&quot;&gt;一文搞明白离散傅里叶变换之作用和威力！ - 数理爱好者的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;有公式推导&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/22611929/answer/341436331&quot;&gt;二维傅里叶变换是怎么进行的？ - CharlyGordon的回答 - 知乎&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/68713342&quot;&gt;如何评价球面卷积神经网络(Spherical CNNs)？ - 知乎&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/660428438&quot;&gt;球面傅立叶变换（SFT，Spherical Fourier Transform） - destroyer的文章 - 知乎&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;三维曲面映射&quot;&gt;三维曲面映射&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ilovematlab.cn/thread-288157-1-1.html?_dsign=f69799a5&quot;&gt;已知离散点坐标，拟合成封闭曲面&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/607755371&quot;&gt;三维散点图曲面拟合 - 奶香小饼干的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;未解决&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pages.mtu.edu/~shene/COURSES/cs3621/NOTES/surface/bspline-construct.html&quot;&gt;mtu 教学&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/qq_37340588/article/details/128181123&quot;&gt;B样条曲线 - CSDN&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;$R_1\rightarrow R_2$, $R_1\rightarrow R_3$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/528642680&quot;&gt;B样条曲线。 - 结庐在人境的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;公式多，可以自己代入运算&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/supuo/b-spline&quot;&gt;球面拟合+B样条&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/468238804&quot;&gt;计算机图形学基础 - 纹理映射 - pozero的文章 - 知乎&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;球面映射&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.csdn.net/pizibing880909/article/details/28910835&quot;&gt;图形学领域的关键算法及源码链接&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;包含一大堆代码&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.numerical-tours.com/matlab/meshdeform_2_parameterization_sphere/&quot;&gt;Spherical Mesh Parameterization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;matlab代码&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;重要&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/garyptchoi/spherical-conformal-map#spherical-conformal-map&quot;&gt;spherical-conformal-map 2017 - git&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;matlab代码&lt;/li&gt;
      &lt;li&gt;比上面的快&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>麦丽素</name></author><category term="AI" /><category term="3d" /><summary type="html">参数化 Parameterization</summary></entry></feed>